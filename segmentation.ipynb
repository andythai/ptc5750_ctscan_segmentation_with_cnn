{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRCADb Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#IRCADb-Segmentation\" data-toc-modified-id=\"IRCADb-Segmentation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>IRCADb Segmentation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Resources</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Code</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* Article: \n",
    "    * https://medium.com/@fabio.sancinetti/u-net-convnet-for-ct-scan-segmentation-6cc0d465eed3\n",
    "* Dataset:\n",
    "    * https://www.ircad.fr/research/3d-ircadb-01/\n",
    "* CUDA:\n",
    "    * https://stackoverflow.com/questions/51111954/tensorflow-cant-find-cudart64-90-dll-even-though-it-is-installed-with-path-va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports jtplot submodule from jupyterthemes, sets notebook theme for plots.\n",
    "# Nice to have if you are using jupyterthemes, otherwise ignore this. \n",
    "try:\n",
    "    __import__('imp').find_module('jupyterthemes')\n",
    "    from jupyterthemes import jtplot\n",
    "    jtplot.style()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# General imports\n",
    "import os, re\n",
    "import pydicom as dicom\n",
    "from scipy.ndimage.interpolation import zoom        \n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine learning imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.models import Model, Sequential\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, Conv2DTranspose, ZeroPadding2D\n",
    "from keras.optimizers import Adam, Adadelta, Adamax, Nadam, Adagrad, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.initializers import RandomUniform, RandomNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientData(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    PatientData looks for files of 3DIRCAD database.\n",
    "    This database contains DICOM files and data is split into folders.\n",
    "    PATIENT_DICOM folder contains original original CT Images\n",
    "    MASKS_DICOM contains a list of several folders. Each folder is named\n",
    "    according to the organ highlighted in the masks of the files within.\n",
    "    During PatientData initialization, it will look for the folder pointed at\n",
    "    root_dir and will load files named with same name on MASKS_DICOM/<organ name>/*\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, oi, file_extension=\".dcm\"):\n",
    "        patient_images = {}\n",
    "\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            print('\\nroot:', root)\n",
    "            print('dirs:', dirs)\n",
    "            print('files:', files)\n",
    "            for file in files:\n",
    "                if file.endswith(file_extension):\n",
    "                    if 'PATIENT_DICOM' in root:\n",
    "                        if not patient_images.get(file,None):\n",
    "                            patient_images[file] = {}\n",
    "                        p = os.path.join(root,file)\n",
    "                        patient_images[file]['real'] = p\n",
    "                    elif 'MASKS_DICOM' in root:\n",
    "                        if not patient_images.get(file,None):\n",
    "                            patient_images[file] = {}\n",
    "                        p = os.path.join(root,file)\n",
    "                        rs = re.match('.*MASKS_DICOM/(.*)/.*', str(p))\n",
    "                        \n",
    "                        print('p:', p)\n",
    "                        print('rs:', rs)\n",
    "                        \n",
    "                        patient_images[file][rs.groups()[0]] = p\n",
    "\n",
    "        self.oi = oi\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "        for k,v in patient_images.items():\n",
    "            for k1,v1 in v.items():\n",
    "                if k1 == self.oi:\n",
    "                    self.X.append(v['real'])\n",
    "                    self.Y.append(v1)\n",
    "        \n",
    "        if len(self.X) != len(self.Y):\n",
    "            raise Exception(\"number of input images (%d) does not match number of training samples (%d)\" % \n",
    "                            (len(self.X),len(self.Y)))\n",
    "   \n",
    "            \n",
    "    def normalize(self, img):\n",
    "        arr = img.copy().astype(np.float)\n",
    "        M = np.float(np.max(img))\n",
    "        if M != 0:\n",
    "            arr *= 1./M\n",
    "        return arr\n",
    "    \n",
    "\n",
    "    def add_gauss_noise(self, inp, expected_noise_ratio=0.05):\n",
    "        image = inp.copy()\n",
    "        if len(image.shape) == 2:\n",
    "            row,col= image.shape\n",
    "            ch = 1\n",
    "        else:\n",
    "            row,col,ch= image.shape\n",
    "        mean = 0.\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col)) * expected_noise_ratio\n",
    "        gauss = gauss.reshape(row,col)\n",
    "        noisy = image + gauss\n",
    "        return noisy\n",
    "\n",
    "    def get_data(self, noisy=False, split_part=0.5, resize_side=None, verbose=False):\n",
    "        im_X = []\n",
    "        im_Y = []\n",
    "        for i in range(len(self.X)):\n",
    "            img_x = dicom.read_file(self.X[i]).pixel_array\n",
    "            img_y = dicom.read_file(self.Y[i]).pixel_array\n",
    "            if resize_side != None:\n",
    "                ratio = resize_side / 512.\n",
    "                img_x = zoom(img_x, ratio).copy()\n",
    "                img_y = zoom(img_y, ratio).copy()\n",
    "            img_x = self.normalize(img_x)\n",
    "            img_y = self.normalize(img_y)\n",
    "\n",
    "            if np.sum(img_y) < 5.:\n",
    "                if np.random.randint(1,10) <= 5:\n",
    "                    if verbose:\n",
    "                        print(\"discarding a very zero like image %s (%f)\" % (self.Y[i],np.sum(img_y)))\n",
    "                    continue\n",
    "            if noisy:\n",
    "                img_x = self.add_gauss_noise(img_x)\n",
    "                img_y = self.add_gauss_noise(img_y)\n",
    "            im_X.append(img_x)\n",
    "            im_Y.append(img_y)\n",
    "            \n",
    "        train_limit = int(len(im_X)*split_part)\n",
    "\n",
    "        indexes = list(range(len(im_X)))\n",
    "        shuffle(indexes)            \n",
    "\n",
    "        shuffleX = [im_X[c] for c in indexes]\n",
    "        shuffleY = [im_Y[c] for c in indexes]\n",
    "\n",
    "        train_x = shuffleX[0:train_limit]\n",
    "        test_x = shuffleX[train_limit:]\n",
    "        train_y = shuffleY[0:train_limit]\n",
    "        test_y = shuffleY[train_limit:]\n",
    "        \n",
    "        return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "root: data/3Dircadb1.2\n",
      "dirs: ['LABELLED_DICOM', 'MASKS_DICOM', 'MESHES_VTK', 'PATIENT_DICOM']\n",
      "files: ['Creative Commons Attribution.pdf', 'Creative Commons Legal Code.pdf', 'LABELLED_DICOM.zip', 'liver_02.jpg', 'MASKS_DICOM.zip', 'MESHES_VTK.zip', 'PATIENT_DICOM.zip']\n",
      "\n",
      "root: data/3Dircadb1.2\\LABELLED_DICOM\n",
      "dirs: []\n",
      "files: ['image_0', 'image_1', 'image_10', 'image_100', 'image_101', 'image_102', 'image_103', 'image_104', 'image_105', 'image_106', 'image_107', 'image_108', 'image_109', 'image_11', 'image_110', 'image_111', 'image_112', 'image_113', 'image_114', 'image_115', 'image_116', 'image_117', 'image_118', 'image_119', 'image_12', 'image_120', 'image_121', 'image_122', 'image_123', 'image_124', 'image_125', 'image_126', 'image_127', 'image_128', 'image_129', 'image_13', 'image_130', 'image_131', 'image_132', 'image_133', 'image_134', 'image_135', 'image_136', 'image_137', 'image_138', 'image_139', 'image_14', 'image_140', 'image_141', 'image_142', 'image_143', 'image_144', 'image_145', 'image_146', 'image_147', 'image_148', 'image_149', 'image_15', 'image_150', 'image_151', 'image_152', 'image_153', 'image_154', 'image_155', 'image_156', 'image_157', 'image_158', 'image_159', 'image_16', 'image_160', 'image_161', 'image_162', 'image_163', 'image_164', 'image_165', 'image_166', 'image_167', 'image_168', 'image_169', 'image_17', 'image_170', 'image_171', 'image_18', 'image_19', 'image_2', 'image_20', 'image_21', 'image_22', 'image_23', 'image_24', 'image_25', 'image_26', 'image_27', 'image_28', 'image_29', 'image_3', 'image_30', 'image_31', 'image_32', 'image_33', 'image_34', 'image_35', 'image_36', 'image_37', 'image_38', 'image_39', 'image_4', 'image_40', 'image_41', 'image_42', 'image_43', 'image_44', 'image_45', 'image_46', 'image_47', 'image_48', 'image_49', 'image_5', 'image_50', 'image_51', 'image_52', 'image_53', 'image_54', 'image_55', 'image_56', 'image_57', 'image_58', 'image_59', 'image_6', 'image_60', 'image_61', 'image_62', 'image_63', 'image_64', 'image_65', 'image_66', 'image_67', 'image_68', 'image_69', 'image_7', 'image_70', 'image_71', 'image_72', 'image_73', 'image_74', 'image_75', 'image_76', 'image_77', 'image_78', 'image_79', 'image_8', 'image_80', 'image_81', 'image_82', 'image_83', 'image_84', 'image_85', 'image_86', 'image_87', 'image_88', 'image_89', 'image_9', 'image_90', 'image_91', 'image_92', 'image_93', 'image_94', 'image_95', 'image_96', 'image_97', 'image_98', 'image_99']\n",
      "\n",
      "root: data/3Dircadb1.2\\MASKS_DICOM\n",
      "dirs: ['bone', 'gallbladder', 'liver', 'livertumor', 'portalvein', 'skin', 'venacava']\n",
      "files: []\n",
      "\n",
      "root: data/3Dircadb1.2\\MASKS_DICOM\\bone\n",
      "dirs: []\n",
      "files: ['image_0', 'image_1', 'image_10', 'image_100', 'image_101', 'image_102', 'image_103', 'image_104', 'image_105', 'image_106', 'image_107', 'image_108', 'image_109', 'image_11', 'image_110', 'image_111', 'image_112', 'image_113', 'image_114', 'image_115', 'image_116', 'image_117', 'image_118', 'image_119', 'image_12', 'image_120', 'image_121', 'image_122', 'image_123', 'image_124', 'image_125', 'image_126', 'image_127', 'image_128', 'image_129', 'image_13', 'image_130', 'image_131', 'image_132', 'image_133', 'image_134', 'image_135', 'image_136', 'image_137', 'image_138', 'image_139', 'image_14', 'image_140', 'image_141', 'image_142', 'image_143', 'image_144', 'image_145', 'image_146', 'image_147', 'image_148', 'image_149', 'image_15', 'image_150', 'image_151', 'image_152', 'image_153', 'image_154', 'image_155', 'image_156', 'image_157', 'image_158', 'image_159', 'image_16', 'image_160', 'image_161', 'image_162', 'image_163', 'image_164', 'image_165', 'image_166', 'image_167', 'image_168', 'image_169', 'image_17', 'image_170', 'image_171', 'image_18', 'image_19', 'image_2', 'image_20', 'image_21', 'image_22', 'image_23', 'image_24', 'image_25', 'image_26', 'image_27', 'image_28', 'image_29', 'image_3', 'image_30', 'image_31', 'image_32', 'image_33', 'image_34', 'image_35', 'image_36', 'image_37', 'image_38', 'image_39', 'image_4', 'image_40', 'image_41', 'image_42', 'image_43', 'image_44', 'image_45', 'image_46', 'image_47', 'image_48', 'image_49', 'image_5', 'image_50', 'image_51', 'image_52', 'image_53', 'image_54', 'image_55', 'image_56', 'image_57', 'image_58', 'image_59', 'image_6', 'image_60', 'image_61', 'image_62', 'image_63', 'image_64', 'image_65', 'image_66', 'image_67', 'image_68', 'image_69', 'image_7', 'image_70', 'image_71', 'image_72', 'image_73', 'image_74', 'image_75', 'image_76', 'image_77', 'image_78', 'image_79', 'image_8', 'image_80', 'image_81', 'image_82', 'image_83', 'image_84', 'image_85', 'image_86', 'image_87', 'image_88', 'image_89', 'image_9', 'image_90', 'image_91', 'image_92', 'image_93', 'image_94', 'image_95', 'image_96', 'image_97', 'image_98', 'image_99']\n",
      "p: data/3Dircadb1.2\\MASKS_DICOM\\bone\\image_0\n",
      "rs: None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-06df058e5cce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mpd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPatientData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bone'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_extension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Also works for 'liver'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_width\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Using 75% for training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-50f48f103cb6>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root_dir, oi, file_extension)\u001b[0m\n\u001b[0;32m     34\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rs:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                         \u001b[0mpatient_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'groups'"
     ]
    }
   ],
   "source": [
    "root_dir = \"data/3Dircadb1.2\"\n",
    "\n",
    "sample_height, sample_width = (128,128)\n",
    "output_width, output_height = (128, 128)\n",
    "img_tot_size = sample_width*sample_height\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "pd = PatientData(root_dir, 'bone', file_extension=\"\") # Also works for 'liver'\n",
    "\n",
    "data = pd.get_data(False, 0.75, sample_width) # Using 75% for training\n",
    "train_x, train_y, test_x, test_y = map(np.array, data)\n",
    "print(\"Using %s images for training and %s images for testing\" % (len(train_x), len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying some samples of the input\n",
    "lines = min(4,len(train_x)) # len(imgs_mask_test) #16\n",
    "fig, axarr = plt.subplots(lines, 2, figsize=(60,lines*10), sharex=True, sharey=False)\n",
    "\n",
    "tot_dice = 0.\n",
    "for i in range(0,lines):\n",
    "    \n",
    "    axarr[i,0].imshow(train_x[i].reshape(output_width, output_height), cmap='gray')\n",
    "    axarr[i,1].imshow(train_y[i].reshape(output_width, output_height), cmap='gray')\n",
    "    \n",
    "    for x in range(2):\n",
    "        axarr[i,x].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.975)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([t.reshape(sample_width, sample_height,1) for t in train_x])\n",
    "train_y = np.array([t.reshape(sample_width, sample_height,1) for t in train_y])\n",
    "test_x  = np.array([t.reshape(sample_width, sample_height,1) for t in test_x])\n",
    "test_y  = np.array([t.reshape(sample_width, sample_height,1) for t in test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizer, loss_metric, metrics, lr=1e-3):\n",
    "    inputs = Input((sample_width, sample_height, 1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    drop1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(drop1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    drop2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(drop2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    drop3 = Dropout(0.3)(pool3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(drop3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    drop4 = Dropout(0.3)(pool4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(drop4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    # working with dice and adam\n",
    "    model.compile(optimizer=Adam(lr=1e-3, decay=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #Adamax\n",
    "    #model.compile(optimizer=Adamax(lr=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #SGD\n",
    "    #model.compile(optimizer=SGD(lr=1e-2, momentum=0.8), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #Nadam\n",
    "    #model.compile(optimizer=Nadam(lr=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #Adadelta\n",
    "    #model.compile(optimizer=Nadam(lr=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "# Dice Coefficient to work with Tensorflow\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "# Dice Coefficient to work outside Tensorflow\n",
    "def dice_coef_2(y_true, y_pred):\n",
    "    side = len(y_true[0])\n",
    "    y_true_f = y_true.reshape(side*side)\n",
    "    y_pred_f = y_pred.reshape(side*side)\n",
    "    intersection = sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (sum(y_true_f) + sum(y_pred_f) + smooth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(optimizer=Adam, loss_metric=dice_coef_loss, metrics=[dice_coef], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_var = 'dice_coef'\n",
    "strategy = 'max' # greater dice_coef is better\n",
    "\n",
    "#observe_var = 'loss' # for binary crossentropy \n",
    "#strategy = 'in' # smallest loss is better\n",
    "\n",
    "#model_checkpoint = ModelCheckpoint('ptc5750.h5', monitor=observe_var, save_best_only=True)\n",
    "#model_reset = ObserveTrainingFailure(observe_var,strategy, -1 , 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the DICOM files\n",
    "DICOM_DATA = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
