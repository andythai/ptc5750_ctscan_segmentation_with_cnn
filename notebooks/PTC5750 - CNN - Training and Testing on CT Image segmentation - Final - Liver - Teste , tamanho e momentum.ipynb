{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentação usando redes neurais tipo C.N.N.\n",
    "\n",
    "## Universidade de São Paulo - Escola Politécnica - PPGEE\n",
    "\n",
    "**Disciplina: PTC5750 / 2017 - 3º Período**\n",
    "\n",
    "**Docente Responsável: Sérgio Shiguemi Furuie**\n",
    "\n",
    "Aluno: Fábio T. Sancinetti - N. USP: 10156476\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, Conv2DTranspose, ZeroPadding2D\n",
    "\n",
    "from keras.optimizers import Adam, Adadelta, Adamax, Nadam, Adagrad, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "from keras.initializers import RandomUniform, RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientData(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    PatientData looks for files of 3DIRCAD database.\n",
    "    This database contains DICOM files and data is split into folders.\n",
    "    PATIENT_DICOM folder contains original original CT Images\n",
    "    MASKS_DICOM contains a list of several folders. Each folder is named\n",
    "    according to the organ highlighted in the masks of the files within.\n",
    "    During PatientData initialization, it will look for the folder pointed at\n",
    "    root_dir and will load files named with same name on MASKS_DICOM/<organ name>/*\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, oi, file_extension=\".dcm\"):\n",
    "        import os, re\n",
    "        \n",
    "        patient_images = {}\n",
    "\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(file_extension):\n",
    "                    if 'PATIENT_DICOM' in root:\n",
    "                        if not patient_images.get(file,None):\n",
    "                            patient_images[file] = {}\n",
    "                        p = os.path.join(root,file)\n",
    "                        patient_images[file]['real'] = p\n",
    "                    elif 'MASKS_DICOM' in root:\n",
    "                        if not patient_images.get(file,None):\n",
    "                            patient_images[file] = {}\n",
    "                        p = os.path.join(root,file)\n",
    "                        rs = re.match('.*MASKS_DICOM/(.*)/.*', str(p))\n",
    "                        patient_images[file][rs.groups()[0]] = p\n",
    "\n",
    "        self.oi = oi\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "        for k,v in patient_images.items():\n",
    "            for k1,v1 in v.items():\n",
    "                if k1 == self.oi:\n",
    "                    self.X.append(v['real'])\n",
    "                    self.Y.append(v1)\n",
    "        \n",
    "        if len(self.X) != len(self.Y):\n",
    "            raise Exception(\"number of input images (%d) does not match number of training samples (%d)\" % \n",
    "                            (len(self.X),len(self.Y)))\n",
    "   \n",
    "            \n",
    "    def normalize(self, img):\n",
    "        arr = img.copy().astype(np.float)\n",
    "        M = np.float(np.max(img))\n",
    "        if M != 0:\n",
    "            arr *= 1./M\n",
    "        return arr\n",
    "    \n",
    "\n",
    "    def add_gauss_noise(self, inp, expected_noise_ratio=0.05):\n",
    "        image = inp.copy()\n",
    "        if len(image.shape) == 2:\n",
    "            row,col= image.shape\n",
    "            ch = 1\n",
    "        else:\n",
    "            row,col,ch= image.shape\n",
    "        mean = 0.\n",
    "        var = 0.1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col)) * expected_noise_ratio\n",
    "        gauss = gauss.reshape(row,col)\n",
    "        noisy = image + gauss\n",
    "        return noisy\n",
    "\n",
    "    def get_data(self, noisy=False, split_part=0.5, resize_side=None, verbose=False):\n",
    "        from scipy.ndimage.interpolation import zoom        \n",
    "        from random import shuffle\n",
    "        import dicom\n",
    "        \n",
    "        im_X = []\n",
    "        im_Y = []\n",
    "        for i in range(len(self.X)):\n",
    "            img_x = dicom.read_file(self.X[i]).pixel_array\n",
    "            img_y = dicom.read_file(self.Y[i]).pixel_array\n",
    "            if resize_side != None:\n",
    "                ratio = resize_side / 512.\n",
    "                img_x = zoom(img_x, ratio).copy()\n",
    "                img_y = zoom(img_y, ratio).copy()\n",
    "            img_x = self.normalize(img_x)\n",
    "            img_y = self.normalize(img_y)\n",
    "\n",
    "            if np.sum(img_y) < 5.:\n",
    "                if np.random.randint(1,10) <= 5:\n",
    "                    if verbose:\n",
    "                        print(\"discarding a very zero like image %s (%f)\" % (self.Y[i],np.sum(img_y)))\n",
    "                    continue\n",
    "            if noisy:\n",
    "                img_x = self.add_gauss_noise(img_x)\n",
    "                img_y = self.add_gauss_noise(img_y)\n",
    "            im_X.append(img_x)\n",
    "            im_Y.append(img_y)\n",
    "            \n",
    "        train_limit = int(len(im_X)*split_part)\n",
    "\n",
    "        indexes = list(range(len(im_X)))\n",
    "        shuffle(indexes)            \n",
    "\n",
    "        shuffleX = [im_X[c] for c in indexes]\n",
    "        shuffleY = [im_Y[c] for c in indexes]\n",
    "\n",
    "        train_x = shuffleX[0:train_limit]\n",
    "        test_x = shuffleX[train_limit:]\n",
    "        train_y = shuffleY[0:train_limit]\n",
    "        test_y = shuffleY[train_limit:]\n",
    "        \n",
    "        return train_x, train_y, test_x, test_y \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/fox/workspace/scipy/projects/dicom/ircad/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_height, sample_width = (128,128)\n",
    "output_width, output_height = (128, 128)\n",
    "img_tot_size = sample_width*sample_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = PatientData(root_dir, 'liver') # Also works for 'liver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_data(False, 0.75, sample_width) # Using 75% for training\n",
    "train_x, train_y, test_x, test_y = map(np.array, data)\n",
    "print(\"Using %s images for training and %s images for testing\" % (len(train_x), len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying some samples of the input\n",
    "lines = min(4,len(train_x)) # len(imgs_mask_test) #16\n",
    "fig, axarr = plt.subplots(lines, 2, figsize=(60,lines*10), sharex=True, sharey=False)\n",
    "\n",
    "tot_dice = 0.\n",
    "for i in range(0,lines):\n",
    "    \n",
    "    axarr[i,0].imshow(train_x[i].reshape(output_width, output_height), cmap='gray')\n",
    "    axarr[i,1].imshow(train_y[i].reshape(output_width, output_height), cmap='gray')\n",
    "    \n",
    "    for x in range(2):\n",
    "        axarr[i,x].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.975)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([t.reshape(sample_width, sample_height,1) for t in train_x])\n",
    "train_y = np.array([t.reshape(sample_width, sample_height,1) for t in train_y])\n",
    "test_x  = np.array([t.reshape(sample_width, sample_height,1) for t in test_x])\n",
    "test_y  = np.array([t.reshape(sample_width, sample_height,1) for t in test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizer, loss_metric, metrics, lr=1e-3):\n",
    "    inputs = Input((sample_width, sample_height, 1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    drop1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(drop1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    drop2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(drop2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    drop3 = Dropout(0.3)(pool3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(drop3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    drop4 = Dropout(0.3)(pool4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(drop4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    # working with dice and adam\n",
    "    #model.compile(optimizer=Adam(lr=1e-3, decay=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #Adamax\n",
    "    #model.compile(optimizer=Adam(lr=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #SGD\n",
    "    #model.compile(optimizer=SGD(lr=1e-1, momentum=0.8), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #Nadam\n",
    "    #model.compile(optimizer=Nadam(lr=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #Adadelta\n",
    "    model.compile(optimizer=Nadam(lr=1e-3), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "# Dice Coefficient to work with Tensorflow\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "# Dice Coefficient to work outside Tensorflow\n",
    "def dice_coef_2(y_true, y_pred):\n",
    "    side = len(y_true[0])\n",
    "    y_true_f = y_true.reshape(side*side)\n",
    "    y_pred_f = y_pred.reshape(side*side)\n",
    "    intersection = sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (sum(y_true_f) + sum(y_pred_f) + smooth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(optimizer=Adam, loss_metric=dice_coef_loss, metrics=[dice_coef], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_var = 'dice_coef'\n",
    "strategy = 'max' # greater dice_coef is better\n",
    "\n",
    "#observe_var = 'loss' # for binary crossentropy \n",
    "#strategy = 'in' # smallest loss is better\n",
    "\n",
    "#model_checkpoint = ModelCheckpoint('ptc5750.h5', monitor=observe_var, save_best_only=True)\n",
    "#model_reset = ObserveTrainingFailure(observe_var,strategy, -1 , 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=int(len(train_x)/4),\n",
    "          epochs=150, verbose=1, shuffle=True, validation_split=0.2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_mask_test = model.predict(test_x, verbose=1)\n",
    "\n",
    "tot_dice = 0.\n",
    "dices = []\n",
    "for i in range(len(imgs_mask_test)):\n",
    "    r = dice_coef_2(test_y[i],imgs_mask_test[i])\n",
    "    dices.append(r)\n",
    "    tot_dice += r\n",
    "    \n",
    "avg_dice = tot_dice/float(len(imgs_mask_test))\n",
    "print(avg_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postProcess = True\n",
    "if postProcess:        \n",
    "    avg = np.average(imgs_mask_test)\n",
    "    for k in range(len(imgs_mask_test)):\n",
    "        im = imgs_mask_test[k]\n",
    "        m = np.max(im)\n",
    "        for i in range(len(im)):\n",
    "            for j in range(len(im[i])):\n",
    "                # remove low confidence results helps to increase dice result\n",
    "                if im[i,j] < (0.5):\n",
    "                    im[i,j] = 0.\n",
    "    \n",
    "    tot_dice = 0.\n",
    "    dices = []\n",
    "    for i in range(len(imgs_mask_test)):\n",
    "        r = dice_coef_2(test_y[i],imgs_mask_test[i])\n",
    "        dices.append(r)\n",
    "        tot_dice += r\n",
    "        \n",
    "\n",
    "    avg_dice = tot_dice/float(len(imgs_mask_test))\n",
    "    print(avg_dice)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Displaying some results\n",
    "lines = max(15,len(imgs_mask_test))\n",
    "fig, axarr = plt.subplots(lines, 3, figsize=(60,lines*10), sharex=True, sharey=False)\n",
    "\n",
    "output_width, output_height = (128, 128)\n",
    "for i in range(0,lines):\n",
    "    axarr[i,0].imshow(test_x[i].reshape(output_width, output_height), cmap='gray')\n",
    "    axarr[i,1].imshow(test_y[i].reshape(output_width, output_height), cmap='gray')\n",
    "    axarr[i,2].imshow(imgs_mask_test[i].reshape(output_width, output_height), cmap='gray')\n",
    "    axarr[i,2].set_title('Dice Coef = ' + str(dices[i]),fontsize=45,ha='center')\n",
    "    \n",
    "    for x in range(3):\n",
    "        axarr[i,x].axis('off')\n",
    "\n",
    "fig.suptitle('Dice average = ' + str(avg_dice) ,fontsize=60)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.975)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
